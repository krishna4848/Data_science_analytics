{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "033b7c76",
   "metadata": {},
   "source": [
    "## [CM 7] \n",
    "> by KEVAL PRAJAPATI (20908584)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa66ed5c",
   "metadata": {},
   "source": [
    "## A) Iris Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1e9659",
   "metadata": {},
   "source": [
    "__Using a class of sklearn called GridSearchCV which allows one to compare different parameter of the classifiers by trying all the combination of those parameters and evaluates the model using cross-validation. Once it completes the evaluation it returns the best parameters for the model___\n",
    "\n",
    "\n",
    "\n",
    "Comparing the best stats from previous questions                   \n",
    "- i) __k__ = 10\n",
    "- ii)__Accuracy__ = 95.23%\n",
    "- iii) __F1-score__ = 95.52%\n",
    "- iv) __AUC__ = 99.82 %\n",
    "- v) __algorithm__ = auto\n",
    "- vi) __metrics__ = minkowski\n",
    "- vii) __p__ = 2\n",
    "- viii) __weights__ = uniform\n",
    "\n",
    "\n",
    "Best parameters returned by GridSearch\n",
    "- i) __k__ = 10\n",
    "- ii)__Accuracy__ = 95.80%\n",
    "- iii) __F1-score__ = 95.52%\n",
    "- iv) __AUC__ = 100 %\n",
    "- v) __algorithm__ = ball_tree\n",
    "- vi) __metrics__ = minkowski\n",
    "- vii) __p__ = 2\n",
    "- viii) __weights__ = uniform\n",
    "\n",
    "Hence after the comparions one can conclude for given Iris dataset the KNN model is finely tuned with k(= No. of Neighbors)=10, Minkowski distance mertic of l2 form which is nothing but euclidean distance and uniform weight produces an accuracy of __95.23%__ , F1-score of __95.52%__ and auc score of __100%__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "64a42e4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Size of training set : Features =  (82, 4)  Labels =  (82,)\n",
      "\n",
      " Size of testing set : Features =  (21, 4)  Labels =  (21,)\n",
      "\n",
      " Size of validation set : Features =  (11, 4)  Labels =  (11,)\n",
      "\n",
      " Best parameters:\n",
      "\n",
      "\n",
      " Best accuracy score: 0.9580952380952381\n",
      "\n",
      " Best parameters:  {'algorithm': 'ball_tree', 'metric': 'minkowski', 'n_neighbors': 10, 'p': 2, 'weights': 'uniform'}\n",
      "\n",
      " Best algorithm and neighbors:  KNeighborsClassifier(algorithm='ball_tree', n_neighbors=10)\n",
      "\n",
      " Accuracy on test set:  95.23809523809523\n",
      "\n",
      " Classification report:\n",
      "\n",
      "\n",
      " Classification_report:\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "    Iris-setosa       1.00      1.00      1.00         4\n",
      "Iris-versicolor       0.83      1.00      0.91         5\n",
      " Iris-virginica       1.00      0.92      0.96        12\n",
      "\n",
      "       accuracy                           0.95        21\n",
      "      macro avg       0.94      0.97      0.96        21\n",
      "   weighted avg       0.96      0.95      0.95        21\n",
      "\n",
      "\n",
      " Confusion matrix:\n",
      "[[ 4  0  0]\n",
      " [ 0  5  0]\n",
      " [ 0  1 11]]\n",
      "\n",
      " F1 score: 0.9552042160737813\n",
      "AUC score: 1.0\n"
     ]
    }
   ],
   "source": [
    "# AUTHOR : KEVAL PRAJAPATI\n",
    "# AIM : PARAMETER OPTIMIZATION\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, auc, roc_curve, roc_auc_score,f1_score, classification_report,plot_roc_curve\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "iris_df = pd.read_csv('D:\\COLLEGE\\Waterloo\\TERM 1\\ECE 657A\\Assignment 1\\dataset\\iris_min_max.csv')\n",
    "\n",
    "\n",
    "X = iris_df.iloc[:,:-1].values  # features\n",
    "y = iris_df.iloc[:,4].values   # lables\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.2, random_state = 98)\n",
    "\n",
    "print(\"\\n Size of training set : Features = \", X_train.shape, \" Labels = \", y_train.shape)\n",
    "print(\"\\n Size of testing set : Features = \", X_test.shape, \" Labels = \", y_test.shape)\n",
    "print(\"\\n Size of validation set : Features = \", X_val.shape, \" Labels = \", y_val.shape)\n",
    "\n",
    "# Training the model  on various parameters\n",
    "\n",
    "k = [1,5,10,15,20,25,30,35]\n",
    "metric_options = ['minkowski']\n",
    "weight_options = ['uniform', 'distance']\n",
    "p1 = [1,2]\n",
    "algo = ['ball_tree', 'kd_tree', 'brute']\n",
    "param_grid = dict(n_neighbors = k, weights = weight_options,algorithm  = algo, metric = metric_options, p = p1)\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "grid = GridSearchCV(knn, param_grid, scoring = 'accuracy')\n",
    "grid.fit(X_train_v, y_train_v)\n",
    "\n",
    "# Best scores provided by the GridSearchCV\n",
    "print(\"\\n Best parameters:\\n\")\n",
    "print (\"\\n Best accuracy score:\",grid.best_score_)\n",
    "print (\"\\n Best parameters: \",grid.best_params_)\n",
    "print (\"\\n Best algorithm and neighbors: \",grid.best_estimator_)\n",
    "\n",
    "# Training and testing the model on the parameters returned by GridSearchCV\n",
    "\n",
    "knn_b = KNeighborsClassifier(10,weights = 'uniform',metric='minkowski',p=2, algorithm = 'ball_tree')\n",
    "knn_b.fit(X_train_v,y_train_v)\n",
    "y_pred_t = knn_b.predict(X_test)\n",
    "accuracy_t= accuracy_score(y_test, y_pred_t)*100\n",
    "print(\"\\n Accuracy on test set: \",accuracy_t)\n",
    "print(\"\\n Classification report:\\n\")\n",
    "print('\\n Classification_report:')\n",
    "print(classification_report(y_test, y_pred_t))\n",
    "print('\\n Confusion matrix:')\n",
    "print(confusion_matrix(y_test, y_pred_t))\n",
    "print(\"\\n F1 score:\",f1_score(y_test,y_pred_t, average='macro'))\n",
    "y_prob = knn_b.predict_proba(X_test)\n",
    "#fpr, tpr, thresh = roc_curve(y_test, y_prob)\n",
    "auc = roc_auc_score(y_test, y_prob, multi_class='ovr')\n",
    "print('AUC score:', auc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69935bf9",
   "metadata": {},
   "source": [
    "## B) Heart Disease Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "373a3d16",
   "metadata": {},
   "source": [
    "Comparing the best stats from previous questions                   \n",
    "- i) __k__ = 5\n",
    "- ii)__Accuracy__ = 85.0%\n",
    "- iii) __F1-score__ = 87.5%\n",
    "- iv) __AUC__ = 91.92 %\n",
    "- v) __algorithm__ = auto\n",
    "- vi) __metrics__ = minkowski\n",
    "- vii) __p__ = 2\n",
    "- viii) __weights__ = uniform\n",
    "\n",
    "\n",
    "Best parameters returned by GridSearch\n",
    "- i) __k__ = 5\n",
    "- ii)__Accuracy__ = 95.0%\n",
    "- iii) __F1-score__ = 94.6%\n",
    "- iv) __AUC__ = 96.3 %\n",
    "- v) __algorithm__ = ball_tree\n",
    "- vi) __metrics__ = minkowski\n",
    "- vii) __p__ = 1\n",
    "- viii) __weights__ = distane\n",
    "\n",
    "Hence after the comparions one can conclude for given Heart Disease dataset the KNN model is finely tuned with k(= No. of Neighbors)=5, Minkowski distance mertic of l1 form which is nothing but manhattan distance and distance based weight produces an accuracy of __95.0%__ , F1-score of __94.6%__ and auc score of __96.3%__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "7859ed29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Size of training set : Features =  (156, 13)  Labels =  (156,)\n",
      "\n",
      " Size of testing set : Features =  (40, 13)  Labels =  (40,)\n",
      "\n",
      " Size of validation set : Features =  (11, 4)  Labels =  (11,)\n",
      "\n",
      " Best parameters:\n",
      "\n",
      "\n",
      " Best accuracy score: 0.8584677419354838\n",
      "\n",
      " Best parameters:  {'algorithm': 'auto', 'metric': 'minkowski', 'n_neighbors': 10, 'p': 1, 'weights': 'distance'}\n",
      "\n",
      " Best algorithm and neighbors:  KNeighborsClassifier(n_neighbors=10, p=1, weights='distance')\n",
      "\n",
      " Accuracy on test set:  95.0\n",
      "\n",
      " Classification report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.88      0.93        16\n",
      "           1       0.92      1.00      0.96        24\n",
      "\n",
      "    accuracy                           0.95        40\n",
      "   macro avg       0.96      0.94      0.95        40\n",
      "weighted avg       0.95      0.95      0.95        40\n",
      "\n",
      "\n",
      " Confusion matrix:\n",
      "[[14  2]\n",
      " [ 0 24]]\n",
      "\n",
      " F1 score: 0.9466666666666668\n",
      "AUC score: 0.9635416666666666\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, auc, roc_curve, roc_auc_score,f1_score, classification_report,plot_roc_curve\n",
    "\n",
    "\n",
    "hd_df = pd.read_csv('D:\\COLLEGE\\Waterloo\\TERM 1\\ECE 657A\\Assignment 1\\dataset\\heart_disease_missing_filled_min_max_final.csv')\n",
    "\n",
    "\n",
    "# Splitting the data set into features and lables\n",
    "X = hd_df.iloc[:,:-1].values  # features\n",
    "y = hd_df.iloc[:,13].values   # lables\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.2, random_state = 98)\n",
    "#X_train_v, X_val, y_train_v, y_val = train_test_split(X_train, y_train, test_size=0.125, random_state=98)\n",
    "\n",
    "print(\"\\n Size of training set : Features = \", X_train.shape, \" Labels = \", y_train.shape)\n",
    "print(\"\\n Size of testing set : Features = \", X_test.shape, \" Labels = \", y_test.shape)\n",
    "print(\"\\n Size of validation set : Features = \", X_val.shape, \" Labels = \", y_val.shape)\n",
    "\n",
    "# Training the model using 5-fold cross validation method \n",
    "\n",
    "k = [1,5,10,15,20,25,30,35]\n",
    "metric_options = ['minkowski']\n",
    "weight_options = ['uniform', 'distance']\n",
    "p1 = [1,2]\n",
    "algo = ['auto','ball_tree', 'kd_tree', 'brute']\n",
    "param_grid = dict(n_neighbors = k, weights = weight_options,algorithm  = algo, metric = metric_options, p = p1)\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "grid = GridSearchCV(knn, param_grid, scoring = 'accuracy')\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(\"\\n Best parameters:\\n\")\n",
    "print (\"\\n Best accuracy score:\",grid.best_score_)\n",
    "print (\"\\n Best parameters: \",grid.best_params_)\n",
    "print (\"\\n Best algorithm and neighbors: \",grid.best_estimator_)\n",
    "\n",
    "# Training and testing the model on the parameters returned by GridSearchCV\n",
    "knn_b = KNeighborsClassifier(10,weights = 'distance',metric='minkowski',p=1, algorithm = 'ball_tree')\n",
    "knn_b.fit(X_train,y_train)\n",
    "y_pred_t = knn_b.predict(X_test)\n",
    "accuracy_t= accuracy_score(y_test, y_pred_t)*100\n",
    "print(\"\\n Accuracy on test set: \",accuracy_t)\n",
    "print(\"\\n Classification report:\\n\")\n",
    "\n",
    "print(classification_report(y_test, y_pred_t))\n",
    "print('\\n Confusion matrix:')\n",
    "print(confusion_matrix(y_test, y_pred_t))\n",
    "print(\"\\n F1 score:\", f1_score(y_test,y_pred_t, average='macro'))\n",
    "y_prob = knn_b.predict_proba(X_test)[::,1]\n",
    "fpr, tpr, thresh = roc_curve(y_test, y_prob)\n",
    "auc = roc_auc_score(y_test, y_prob)\n",
    "print('AUC score:', auc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc69d028",
   "metadata": {},
   "source": [
    "__References__\n",
    "- https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html\n",
    "- https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html\n",
    "- https://towardsdatascience.com/ways-to-detect-and-remove-the-outliers-404d16608dba\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
